{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestRegressor# Instantiate model with 1000 decision trees\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import  matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing(df):\n",
    "    missing = df.isna().sum()\n",
    "    missing /= df.shape[0]\n",
    "    missing *=100\n",
    "    missing = missing.to_frame().rename(columns={0:'Percent Of Missing Values'})\n",
    "    return missing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"datasets/Institutional Finances/F_F2_1415-1920_data.csv\"\n",
    "institutional_finance = pd.read_csv(filepath, index_col=0, low_memory= False)\n",
    "missing = institutional_finance.isna().sum()\n",
    "missing /= institutional_finance.shape[0]\n",
    "missing *=100\n",
    "missing = missing.to_frame().rename(columns={0:'Percent Of Missing Values'})\n",
    "threshold = 30\n",
    "institutional_finance_over_threshold_missing = institutional_finance[(missing[missing[\"Percent Of Missing Values\"] < threshold]).index]\n",
    "missing = institutional_finance_over_threshold_missing.isna().sum()\n",
    "missing /= institutional_finance_over_threshold_missing.shape[0]\n",
    "missing *=100\n",
    "missing = missing.to_frame().rename(columns={0:'Percent Of Missing Values'})\n",
    "# All missing entries with percent missing > 7 are verifiably 0 imputable\n",
    "missing_keys = (missing[missing[\"Percent Of Missing Values\"] > 7]).index\n",
    "imputation_dict = {k: 0 for k in missing_keys}\n",
    "\n",
    "cleaned_institutional_finance = institutional_finance_over_threshold_missing.fillna(imputation_dict)\n",
    "\n",
    "# Remaining missing values are mean imputed -- very small fraction of rows in any case\n",
    "num_cols = cleaned_institutional_finance.columns[cleaned_institutional_finance.dtypes.values != 'object']\n",
    "cleaned_institutional_finance = institutional_finance_over_threshold_missing.fillna(value=cleaned_institutional_finance[num_cols].mean())\n",
    "cleaned_institutional_finance = cleaned_institutional_finance[num_cols]\n",
    "cleaned_institutional_finance.drop(columns= \"f2d17\", inplace = True)\n",
    "sigma_threshold = 2\n",
    "cleaned_institutional_finance_no_outliers = cleaned_institutional_finance[(np.abs(stats.zscore(cleaned_institutional_finance)) < sigma_threshold).all(axis=1)]\n",
    "\n",
    "fnce_data_unique = cleaned_institutional_finance_no_outliers.groupby(['unitid']).mean().reset_index()\n",
    "fnce_data_final = fnce_data_unique.drop(['year'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_expense():\n",
    "    train_features = fnce_data_final.drop(['unitid'], axis = 1)\n",
    "    model = PCA(n_components=train_features.shape[1]).fit(train_features)\n",
    "    X_pc = model.transform(train_features)\n",
    "\n",
    "    # number of components\n",
    "    n_pcs= model.components_.shape[0]\n",
    "\n",
    "    # get the index of the most important feature on EACH component\n",
    "    # LIST COMPREHENSION HERE\n",
    "    most_important = [np.abs(model.components_[i]).argmax() for i in range(n_pcs)]\n",
    "\n",
    "    initial_feature_names = train_features.columns\n",
    "    # get the names\n",
    "    most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n",
    "\n",
    "    # LIST COMPREHENSION HERE AGAIN\n",
    "    dic = {'PC{}'.format(i): most_important_names[i] for i in range(n_pcs)}\n",
    "    expenses = ['unitid']\n",
    "    seen_categories = ['13']\n",
    "    for i in range(n_pcs):\n",
    "        most_important_name = most_important_names[i]\n",
    "        category = most_important_name[3:5]\n",
    "\n",
    "        if 'e' in most_important_name and most_important_name != 'unitid' and most_important_name not in expenses and category not in seen_categories:\n",
    "            # print(f\"name: {most_important_name}, cat: {category}\")\n",
    "            expenses.append(most_important_name)\n",
    "            seen_categories.append(category)\n",
    "    # expenses.append('unitid')\n",
    "    expenses = np.array(expenses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "expenses = ['unitid', 'f2b02', 'f2c05', 'f2c06', 'f2d11', 'f2d12', 'f2e011', 'f2e021', 'f2e031', 'f2e041', 'f2e051', 'f2e061', 'f2e071', 'f2e081']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dragonblade62\\AppData\\Local\\Temp\\ipykernel_14188\\927545629.py:2: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  total_students = total_students.groupby('unitid').mean().reset_index()\n"
     ]
    }
   ],
   "source": [
    "total_students = pd.read_csv(\"datasets/12-Month Enrollment/EFFY_2015-2021_data.csv\", low_memory = False)\n",
    "total_students = total_students.groupby('unitid').mean().reset_index()\n",
    "total_students = total_students[['unitid', 'efytotlt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dragonblade62\\AppData\\Local\\Temp\\ipykernel_14188\\2997873170.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fnce_data_final_features[c] /= fnce_data_final['f2e131']\n"
     ]
    }
   ],
   "source": [
    "fnce_data_final_features = fnce_data_final[expenses]\n",
    "for c in fnce_data_final_features.columns:\n",
    "    if c != 'unitid':\n",
    "        fnce_data_final_features[c] /= fnce_data_final['f2e131']\n",
    "for c in fnce_data_final_features.columns:\n",
    "    if c!= 'unitid' and 'e' in c:\n",
    "        fnce_data_final_features = fnce_data_final_features[fnce_data_final_features[c] < 1]\n",
    "fnce_data_final_features = pd.merge(fnce_data_final_features, total_students, on='unitid', how='inner')\n",
    "fnce_data_final_features['expense_ps'] = fnce_data_final_features['f2b02'] / fnce_data_final_features['efytotlt']\n",
    "fnce_data_final_features = fnce_data_final_features.drop(['efytotlt'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X, y):\n",
    "    # if np.count_nonzero(y == 0) > 0:\n",
    "    #     lr = LinearRegression().fit(X, y)\n",
    "    #     print(lr.score(X,y))\n",
    "    # else:\n",
    "    offset = int(X.shape[0] * 0.85)\n",
    "    X_train, y_train = X[:offset], y[:offset]\n",
    "    X_test, y_test = X[offset:], y[offset:]\n",
    "    reg = LazyRegressor(verbose=1, ignore_warnings=True, custom_metric=None)\n",
    "    models, predictions = reg.fit(X_train, X_test, y_train, y_test)\n",
    "    print(models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_test(X, y, input_frame, name, save_threshold = None, ):\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17)\n",
    "    grid = {\n",
    "        'n_estimators': [3, 5, 7, 10, 15, 20, 30, 40],\n",
    "        'max_depth' : [5, 7, 10, 12, 15, 18],\n",
    "        'eta' : [0.05, 0.1, 0.12, 0.15, 0.17, 0.2, 0.22, 0.25],\n",
    "        'grow_policy' : ['depthwise', 'lossguide']\n",
    "    }\n",
    "\n",
    "    model_s = XGBRegressor()\n",
    "    xgbr_cv_s = RandomizedSearchCV(estimator=model_s, param_distributions=grid, cv=10, n_jobs=-1)\n",
    "    xgbr_cv_s.fit(x_train, y_train)\n",
    "    res_s = xgbr_cv_s.best_params_\n",
    "    print(res_s)\n",
    "    xgbr_s = XGBRegressor(n_estimators=res_s[\"n_estimators\"], max_depth=res_s[\"max_depth\"], eta=res_s[\"eta\"], grow_policy=res_s[\"grow_policy\"], eval_metric='rmse').fit(x_train, y_train)\n",
    "    score = xgbr_s.score(x_test, y_test)\n",
    "    preds = xgbr_s.predict(x_test)\n",
    "    mse = mean_squared_error(y_test, preds)\n",
    "    print(\"Test set R^2 Score: \", score)\n",
    "    print(\"MSE: \", mse)\n",
    "    print(\"RMSE: \", mse**(0.5))\n",
    "    if save_threshold is not None and score >= save_threshold:\n",
    "        xgbr_s.save_model(name+'.json')\n",
    "        input_frame.to_csv(name+'.csv')\n",
    "        print(\"SAVED!\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_set(datapath, target, name, second = None, log = True, div_by_student = True, div_by_expense = True):\n",
    "    df = pd.read_csv(datapath, low_memory=False)\n",
    "    print(target)\n",
    "    df_subset = df[['unitid', target]]\n",
    "    df_subset.dropna(inplace=True, axis = 0)\n",
    "    print(df_subset.shape)\n",
    "    df_subset = df_subset.groupby('unitid').mean().reset_index()\n",
    "    if second != None:\n",
    "        df_subset[target] /= df[second]\n",
    "        df_subset = df_subset.drop([second], axis=1)\n",
    "    if div_by_student:\n",
    "        df_subset = pd.merge(df_subset, total_students, how='inner', on='unitid')\n",
    "        df_subset[target] /= df_subset['efytotlt']\n",
    "        df_subset = df_subset.drop(['efytotlt'], axis = 1)\n",
    "    if div_by_expense:\n",
    "        expenses = fnce_data_final[['f2e131', 'unitid']]\n",
    "        df_subset = pd.merge(df_subset, expenses, on='unitid', how='inner')\n",
    "        print(df_subset.columns)\n",
    "        df_subset[target] /= df_subset['f2e131']\n",
    "        print(df_subset.shape)\n",
    "        df_subset = df_subset.drop(['f2e131'], axis = 1)\n",
    "    if log:\n",
    "        df_subset[target] = np.log(df_subset[target])\n",
    "        df_subset.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        print(get_missing(df_subset))\n",
    "        # df_subset = df_subset.fillna(df_subset.median())\n",
    "        df_subset.dropna(inplace=True)\n",
    "    merged = pd.merge(df_subset, fnce_data_final_features, how='inner', on='unitid')\n",
    "    print(merged.shape)\n",
    "    print(get_missing(merged))\n",
    "    merged.dropna(inplace=True)\n",
    "    \n",
    "    X = merged.drop(['unitid', target], axis = 1)\n",
    "    y = merged[target]\n",
    "    scaler = StandardScaler()\n",
    "    X = pd.DataFrame(scaler.fit_transform(X), columns = X.columns)\n",
    "\n",
    "    for c in X.columns:\n",
    "        merged[c] = X[c]\n",
    "    # y[np.array(np.where(y ==0))] = 1e-9\n",
    "    print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "    xgb_test(X, y, merged, name, 0.75)\n",
    "    # test(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grtotlt\n",
      "(369741, 2)\n",
      "Index(['unitid', 'grtotlt', 'f2e131'], dtype='object')\n",
      "(1395, 3)\n",
      "         Percent Of Missing Values\n",
      "unitid                         0.0\n",
      "grtotlt                        0.0\n",
      "(1334, 16)\n",
      "            Percent Of Missing Values\n",
      "unitid                            0.0\n",
      "grtotlt                           0.0\n",
      "f2b02                             0.0\n",
      "f2c05                             0.0\n",
      "f2c06                             0.0\n",
      "f2d11                             0.0\n",
      "f2d12                             0.0\n",
      "f2e011                            0.0\n",
      "f2e021                            0.0\n",
      "f2e031                            0.0\n",
      "f2e041                            0.0\n",
      "f2e051                            0.0\n",
      "f2e061                            0.0\n",
      "f2e071                            0.0\n",
      "f2e081                            0.0\n",
      "expense_ps                        0.0\n",
      "X shape: (1334, 14), y shape: (1334,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dragonblade62\\AppData\\Local\\Temp\\ipykernel_14188\\1885325124.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subset.dropna(inplace=True, axis = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 40, 'max_depth': 18, 'grow_policy': 'lossguide', 'eta': 0.17}\n",
      "Test set R^2 Score:  0.7524222564097562\n",
      "MSE:  0.6920745352009767\n",
      "RMSE:  0.8319101725552926\n",
      "SAVED!\n"
     ]
    }
   ],
   "source": [
    "test_set(\"datasets/Graduation Rates/GR_2015-2021_data.csv \", 'grtotlt', 'grad_rate_per_dollar', div_by_student=True, div_by_expense=True, log = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "39e3f26245d4e4c77fbded36ef9fcfab8d6b16b727ecaf20aee6abd83b69e71a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
