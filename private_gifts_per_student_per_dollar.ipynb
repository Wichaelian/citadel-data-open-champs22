{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestRegressor# Instantiate model with 1000 decision trees\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import  matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing(df):\n",
    "    missing = df.isna().sum()\n",
    "    missing /= df.shape[0]\n",
    "    missing *=100\n",
    "    missing = missing.to_frame().rename(columns={0:'Percent Of Missing Values'})\n",
    "    return missing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"datasets/Institutional Finances/F_F2_1415-1920_data.csv\"\n",
    "institutional_finance = pd.read_csv(filepath, index_col=0, low_memory= False)\n",
    "missing = institutional_finance.isna().sum()\n",
    "missing /= institutional_finance.shape[0]\n",
    "missing *=100\n",
    "missing = missing.to_frame().rename(columns={0:'Percent Of Missing Values'})\n",
    "threshold = 30\n",
    "institutional_finance_over_threshold_missing = institutional_finance[(missing[missing[\"Percent Of Missing Values\"] < threshold]).index]\n",
    "missing = institutional_finance_over_threshold_missing.isna().sum()\n",
    "missing /= institutional_finance_over_threshold_missing.shape[0]\n",
    "missing *=100\n",
    "missing = missing.to_frame().rename(columns={0:'Percent Of Missing Values'})\n",
    "# All missing entries with percent missing > 7 are verifiably 0 imputable\n",
    "missing_keys = (missing[missing[\"Percent Of Missing Values\"] > 7]).index\n",
    "imputation_dict = {k: 0 for k in missing_keys}\n",
    "\n",
    "cleaned_institutional_finance = institutional_finance_over_threshold_missing.fillna(imputation_dict)\n",
    "\n",
    "# Remaining missing values are mean imputed -- very small fraction of rows in any case\n",
    "num_cols = cleaned_institutional_finance.columns[cleaned_institutional_finance.dtypes.values != 'object']\n",
    "cleaned_institutional_finance = institutional_finance_over_threshold_missing.fillna(value=cleaned_institutional_finance[num_cols].mean())\n",
    "cleaned_institutional_finance = cleaned_institutional_finance[num_cols]\n",
    "cleaned_institutional_finance.drop(columns= \"f2d17\", inplace = True)\n",
    "sigma_threshold = 2\n",
    "cleaned_institutional_finance_no_outliers = cleaned_institutional_finance[(np.abs(stats.zscore(cleaned_institutional_finance)) < sigma_threshold).all(axis=1)]\n",
    "\n",
    "fnce_data_unique = cleaned_institutional_finance_no_outliers.groupby(['unitid']).mean().reset_index()\n",
    "fnce_data_final = fnce_data_unique.drop(['year'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_expense():\n",
    "    train_features = fnce_data_final.drop(['unitid'], axis = 1)\n",
    "    model = PCA(n_components=train_features.shape[1]).fit(train_features)\n",
    "    X_pc = model.transform(train_features)\n",
    "\n",
    "    # number of components\n",
    "    n_pcs= model.components_.shape[0]\n",
    "\n",
    "    # get the index of the most important feature on EACH component\n",
    "    # LIST COMPREHENSION HERE\n",
    "    most_important = [np.abs(model.components_[i]).argmax() for i in range(n_pcs)]\n",
    "\n",
    "    initial_feature_names = train_features.columns\n",
    "    # get the names\n",
    "    most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n",
    "\n",
    "    # LIST COMPREHENSION HERE AGAIN\n",
    "    dic = {'PC{}'.format(i): most_important_names[i] for i in range(n_pcs)}\n",
    "    expenses = ['unitid']\n",
    "    seen_categories = ['13']\n",
    "    for i in range(n_pcs):\n",
    "        most_important_name = most_important_names[i]\n",
    "        category = most_important_name[3:5]\n",
    "\n",
    "        if 'e' in most_important_name and most_important_name != 'unitid' and most_important_name not in expenses and category not in seen_categories:\n",
    "            # print(f\"name: {most_important_name}, cat: {category}\")\n",
    "            expenses.append(most_important_name)\n",
    "            seen_categories.append(category)\n",
    "    # expenses.append('unitid')\n",
    "    expenses = np.array(expenses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "expenses = ['unitid', 'f2b02', 'f2c05', 'f2c06', 'f2d11', 'f2d12', 'f2e011', 'f2e021', 'f2e031', 'f2e041', 'f2e051', 'f2e061', 'f2e071', 'f2e081']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_students = pd.read_csv(\"datasets/12-Month Enrollment/EFFY_2015-2021_data.csv\")\n",
    "total_students = total_students.groupby('unitid').mean().reset_index()\n",
    "total_students = total_students[['unitid', 'efytotlt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnce_data_final_features = fnce_data_final[expenses]\n",
    "for c in fnce_data_final_features.columns:\n",
    "    if c != 'unitid':\n",
    "        fnce_data_final_features[c] /= fnce_data_final['f2e131']\n",
    "for c in fnce_data_final_features.columns:\n",
    "    if c!= 'unitid' and 'e' in c:\n",
    "        fnce_data_final_features = fnce_data_final_features[fnce_data_final_features[c] < 1]\n",
    "fnce_data_final_features = pd.merge(fnce_data_final_features, total_students, on='unitid', how='inner')\n",
    "fnce_data_final_features['expense_ps'] = fnce_data_final_features['f2b02'] / fnce_data_final_features['efytotlt']\n",
    "fnce_data_final_features = fnce_data_final_features.drop(['efytotlt'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X, y):\n",
    "    # if np.count_nonzero(y == 0) > 0:\n",
    "    #     lr = LinearRegression().fit(X, y)\n",
    "    #     print(lr.score(X,y))\n",
    "    # else:\n",
    "    offset = int(X.shape[0] * 0.85)\n",
    "    X_train, y_train = X[:offset], y[:offset]\n",
    "    X_test, y_test = X[offset:], y[offset:]\n",
    "    reg = LazyRegressor(verbose=1, ignore_warnings=True, custom_metric=None)\n",
    "    models, predictions = reg.fit(X_train, X_test, y_train, y_test)\n",
    "    print(models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_test(X, y, input_frame):\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17)\n",
    "    grid = {\n",
    "        'n_estimators': [3, 5, 7, 10, 15, 20, 30, 40],\n",
    "        'max_depth' : [5, 7, 10, 12, 15, 18],\n",
    "        'eta' : [0.05, 0.1, 0.12, 0.15, 0.17, 0.2, 0.22, 0.25],\n",
    "        'grow_policy' : ['depthwise', 'lossguide']\n",
    "    }\n",
    "\n",
    "    model_s = XGBRegressor()\n",
    "    xgbr_cv_s = RandomizedSearchCV(estimator=model_s, param_distributions=grid, cv=10, n_jobs=-1)\n",
    "    xgbr_cv_s.fit(x_train, y_train)\n",
    "    res_s = xgbr_cv_s.best_params_\n",
    "    print(res_s)\n",
    "    xgbr_s = XGBRegressor(n_estimators=res_s[\"n_estimators\"], max_depth=res_s[\"max_depth\"], eta=res_s[\"eta\"], grow_policy=res_s[\"grow_policy\"], eval_metric='rmse').fit(x_train, y_train)\n",
    "    score = xgbr_s.score(x_test, y_test)\n",
    "    preds = xgbr_s.predict(x_test)\n",
    "    mse = mean_squared_error(y_test, preds)\n",
    "    print(\"Test set R^2 Score: \", score)\n",
    "    print(\"MSE: \", mse)\n",
    "    print(\"RMSE: \", mse**(0.5))\n",
    "    if score > 0.7:\n",
    "        xgbr_s.save_model('private_gifts_model.json')\n",
    "        input_frame.to_csv('private_gifts_per_student_per_dollar.csv')\n",
    "        print(\"SAVED!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_set(datapath, target, second = None, log = True, div_by_student = True, div_by_expense = True):\n",
    "    df = pd.read_csv(datapath, low_memory=False)\n",
    "    print(target)\n",
    "    df_subset = df[['unitid', target]]\n",
    "    df_subset.dropna(inplace=True, axis = 0)\n",
    "    print(df_subset.shape)\n",
    "    df_subset = df_subset.groupby('unitid').mean().reset_index()\n",
    "    if second != None:\n",
    "        df_subset[target] /= df[second]\n",
    "        df_subset = df_subset.drop([second], axis=1)\n",
    "    if div_by_student:\n",
    "        df_subset = pd.merge(df_subset, total_students, how='inner', on='unitid')\n",
    "        df_subset[target] /= df_subset['efytotlt']\n",
    "        df_subset = df_subset.drop(['efytotlt'], axis = 1)\n",
    "    if div_by_expense:\n",
    "        expenses = fnce_data_final[['f2e131', 'unitid']]\n",
    "        df_subset = pd.merge(df_subset, expenses, on='unitid', how='inner')\n",
    "        print(df_subset.columns)\n",
    "        df_subset[target] /= df_subset['f2e131']\n",
    "        print(df_subset.shape)\n",
    "        df_subset = df_subset.drop(['f2e131'], axis = 1)\n",
    "    if log:\n",
    "        df_subset[target] = np.log(df_subset[target])\n",
    "        df_subset.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        print(get_missing(df_subset))\n",
    "        # df_subset = df_subset.fillna(df_subset.median())\n",
    "        df_subset.dropna(inplace=True)\n",
    "    merged = pd.merge(df_subset, fnce_data_final_features, how='inner', on='unitid')\n",
    "    print(merged.shape)\n",
    "    print(get_missing(merged))\n",
    "    merged.dropna(inplace=True)\n",
    "    \n",
    "    X = merged.drop(['unitid', target], axis = 1)\n",
    "    y = merged[target]\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    # y[np.array(np.where(y ==0))] = 1e-9\n",
    "    print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "    xgb_test(X, y, merged)\n",
    "    # test(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f2d08\n",
      "(11198, 2)\n",
      "Index(['unitid', 'f2d08', 'f2e131'], dtype='object')\n",
      "(1840, 3)\n",
      "        Percent Of Missing Values\n",
      "unitid                       0.00\n",
      "f2d08                       14.40\n",
      "(1498, 16)\n",
      "            Percent Of Missing Values\n",
      "unitid                           0.00\n",
      "f2d08                            0.00\n",
      "f2b02                            0.00\n",
      "f2c05                            0.00\n",
      "f2c06                            0.00\n",
      "f2d11                            0.00\n",
      "f2d12                            0.00\n",
      "f2e011                           0.00\n",
      "f2e021                           0.00\n",
      "f2e031                           0.00\n",
      "f2e041                           0.00\n",
      "f2e051                           0.00\n",
      "f2e061                           0.00\n",
      "f2e071                           0.00\n",
      "f2e081                           0.00\n",
      "expense_ps                       0.00\n",
      "X shape: (1498, 14), y shape: (1498,)\n",
      "{'n_estimators': 30, 'max_depth': 5, 'grow_policy': 'depthwise', 'eta': 0.15}\n",
      "Test set R^2 Score:  0.7119178821400727\n",
      "MSE:  1.6163887955565879\n",
      "RMSE:  1.2713727995975799\n",
      "SAVED!\n"
     ]
    }
   ],
   "source": [
    "test_set(\"datasets/Institutional Finances/F_F2_1415-1920_data.csv\", 'f2d08', div_by_student=True, div_by_expense=True, log = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "39e3f26245d4e4c77fbded36ef9fcfab8d6b16b727ecaf20aee6abd83b69e71a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
